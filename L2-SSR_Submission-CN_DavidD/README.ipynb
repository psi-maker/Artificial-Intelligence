{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6> Writeup Report for RoboND - L.2. Search and Sample Return </font>\n",
    "- By: David, Dong  \n",
    "- Udacity username: david.dong@psi-maker.com  \n",
    "- Date:  Sep. 12, 2017\n",
    "- Location: Beijing, China\n",
    "---\n",
    "\n",
    "# Project environment setup\n",
    "1. Windows 10 Pro English version 64-bit\n",
    "2. [Windows Simulator Build](https://s3-us-west-1.amazonaws.com/udacity-robotics/Rover+Unity+Sims/Windows_Roversim.zip)\n",
    " - *Screen resolution: 800 x 600*\n",
    " - *Graphic quality: Good*\n",
    " - *Display 1*  \n",
    " - *Video recording rate in simulator is fps=25, output video is fps=50*\n",
    "3. Software\n",
    "  - Anaconda3-4.4.0-Python3.6-Windows-x86_64.exe\n",
    "  conda upgrade conda  \n",
    "  conda upgrade --all\n",
    "  conda install nb_conda (to manage environment)\n",
    "  - Git-2.14.1-64-bit.exe\n",
    "  - Git Clone RoboND-Python-StarterKit\n",
    "  - Create RoboND environment   \n",
    "  - Install Nbextensions for Jupyter *(notebook table of contents)*\n",
    "  - jupyter nbextensions_configurator enable\n",
    " ---\n",
    "# Folders structure\n",
    "> __L2-SSR_Submission-CN_DavidD__\n",
    " >> __calibration_images__ï¼š*images for python code testing*  \n",
    " >> __output__: *Video & picture output by 'test_rover_img_basic.ipynb'*  \n",
    " >> __record_data__: *the simulator(Roversim) training mode recorded image and 'robot_log.csv'*  \n",
    " >> __ref__: *files, code, tools for future improvement*  \n",
    " >> __code__:  \n",
    "  >>> Autonomous navigation scripts (drive_rover.py, supporting_functions.py, decision.py and perception.py)   \n",
    "  >>> run_test.ipynb: to run 'drive_rover.py' in Ipython  \n",
    "  >>> test_perception_py: Python self improvement testing code  \n",
    "  >>> test_rover_img_basic.ipynb:process_image() code*\n",
    "---\n",
    "# Key Knowledge\n",
    "## Dimensions & space \n",
    "- Telemetry Coordinates, Euler angels; naval and aeronautical position  \n",
    "  ![Example](ref/TelemetryCoordinates.JPG)  \n",
    "\n",
    "- Matrix is the dimensions symbol, should review more knowledge about this\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "x' \\\\\n",
    "y'\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "cos(\\theta) & -sin(\\theta) \\\\\n",
    "sin(\\theta) & cos(\\theta)\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$  \n",
    "## Theory & tools for future improvement\n",
    "- Calculus, Algebra \n",
    "- Image Matrix concepts, Image pixels - different numpy arrays, color channels(RGB; BGR; Gray; HSV; HSL), single channel binary image and differnece\n",
    "- Python coding skills, libraries for image & data analysis, matplotlib; numpy; pandas; cv2; Mahotas; PIL; SciPy; etc.\n",
    "- Tool: Anaconda, Python, Jupyter notebook, nbextensions, Atom, Latex, workflow chart by MD\n",
    "- Raw image Coordinates; Rover-Centric Coordinates; map to World Coordinates; polar Coordinates\n",
    "- Layout by plt.subplot(m, n, p)  \n",
    "- The uint8 image can be plotted, but others cannot\n",
    "- Perspective, thresholding(e.g. OTSU, HSV method etc.),border identification, radar scan\n",
    "---\n",
    "# Rubric Points\n",
    "## Notebook Analysis\n",
    "### How to  analyze the image  by Python code\n",
    "Please refer to __'test_rover_img_basic.ipynb'__  \n",
    "   \n",
    " > 1. Three important concepts for image analysis: dimensions' coordinates; image channels; image overlay. Familiar with below python libraries will be helpful:   \n",
    "matplotlib; numpy; pandas; cv2; Mahotas; PIL; SciPy;\n",
    " > 2. Basic image analysis, the image perception step \n",
    "  >> __Perspective__: from a camera image to the top-down view, the scale is 10x10 pixels = 0.1x0.1 meter square. It is still the 3 channel color image after perspective.  \n",
    "  >> __Thresholding__: Seperate color image to single channel binary image  \n",
    "   >>> 1) __Navigation area thresholding__: There are many ways for thresholding. I tested OTSU(*otsu_threshold()*), and static 3-tuple color threshold values in 'test_rover_img_basic.ipynb'. Both are ok for this project, by that we can get the navigation area on binary image with value '1' for navigatiion, and value '0' for other areas. __Note: by OTSU threshold, the result will include the yellow rock; by static color threshold value, the result will not include the yellow rock__  \n",
    "   >>> 2) __Obstacle area thresholding__: obstacle = 1-navigation; or obstacle = cv2.bitwise_not(naigation); or obstacle = ~navigation  \n",
    "   >>> 3) __Rock sample thresholding__: in the functioin rock_thresh() of 'test_rover_img_basic.ipynb', it changes the original BGR image to HSV image by *cv2.cvtColor* method, then use *cv2.inRange(hsv, hsv_lower_yellow, hsv_upper_yellow)* to get the yellow  rock samele mask, the single channel image,at last by integrate the mask image into original image to get three channels color image for rock sample\n",
    "  >>\n",
    "  >> __Image border__: by *'cv2.findContours'*, we can find single channel images' borders for future detailed analysis\n",
    "  >>__Create worldmap__: create the worldmap from a same size PNG file with one square meter per pixel\n",
    " >\n",
    " > 3. Rover-centric map: All thresholding images can be convert to rover-centirc coordinates\n",
    "\n",
    " \n",
    "### World map integration\n",
    "__World map integration__ '*process_image()*':  \n",
    " > in 'test_rover_img_basic.ipynb -- 8.2 Process_image code', I tested image overlay to plot the worldmap image\n",
    "  >> 1) use random rover yaw & postion coordinates x,y; and create the worldmap from a same size PNG file with one square meter per pixel  \n",
    "  >> 2) put the obstacle area binary image to worldmap image channel 0(R channel)  \n",
    "  >> 3) put the rock area binary image to worldmap image channel 1(G channel)  \n",
    "  >> 4) put the navigation area binary image to worldmap image channel 2(B channel)  \n",
    "  >> 5) Integrate the overlay worldmap by '*cv2.addWeighted*'\n",
    " >\n",
    " > put the above test code into the method 'process_image()', replaced the testing rover yaw, rover position & image by simulator recorded values*(robot_log.csv) in the folder 'record_data'* \n",
    "\n",
    "### Output video\n",
    "By 'ffmpeg' plugin & 'moviepy' library output the video in the folder output\\>recordImg2Video_mapping.mp4 for simulator recorded data \n",
    "\n",
    "## Autonomous Navigation and Mapping summary\n",
    "### Information for simulator\n",
    " - *Screen resolution: 800 x 600*\n",
    " - *Graphic quality: Good*\n",
    " - *Display 1*  \n",
    " - FPS output to terminal by drive_rover.py: from 20 to 25  \n",
    " \n",
    "### Script result\n",
    "  \n",
    "__Please run driver_rover_start.ipynb by jupyter notebook for Autonomous Mode__\n",
    "- more than 40% of the world map environment\n",
    "- more than 60% fidelity (accuracy) against the ground truth: Identify the valid image data to improve fidelity(good_nav_pix) in perception.py if (abs(Rover.pitch < 10)) & (abs(Rover.roll < 10))\n",
    "- located more than 1 rock sample\n",
    "- not pickup any rocks\n",
    "- not record any image on autonomous mode\n",
    "\n",
    " \n",
    "### key summary for Autonomous script update \n",
    "1. changed 'max_vel' from 2 to 30 in *decision.py*\n",
    "2. Below parameters are used or added for rover decision.py\n",
    "    ```\n",
    "     self.stop_forward = 50 # Threshold to initiate stopping\n",
    "     self.go_forward = 500  # Threshold to go forward again\n",
    "     self.time_5s = None   # check rover status by interval 5 seconds, to avoid unexpectly rover jam\n",
    "     self.s_pos = None     #  rover position before 5 seconds\n",
    "     self.barrier = 0     # Radar Scan(*perception.py - radar_scan()*) function to check 3 meters(less than rock identify distance) ahead the rover navigation area. Stop the rover if there is rocks or large barrier in front of rover in *decision.py*.\n",
    "    ```\n",
    "3. Identify the valid image data to improve fidelity(good_nav_pix) in *perception.py* \n",
    "    ```\n",
    "    if (abs(Rover.pitch < 10)) & (abs(Rover.roll < 10))\n",
    "    ``` \n",
    "4. *Decision.py - decision_step(Rover)* update\n",
    " ![rover_path](ref/Decision_Tree.JPG) \n",
    " __Desicion tree description__\n",
    " - Rover go along with the right direction `Rover.steer = np.clip(np.mean(Rover.nav_angles * 180/np.pi), -15, 0)` on FORWARD status\n",
    " - Stop if not enough going forward navigation area pixels\n",
    " - Stop the rover if there are rocks or large barrier in front of rover every 5 seconds\n",
    " - Turn left once the rover is facing the wall or rocks, untill -15\\<steer angel\\<15\n",
    "5. update the self.time_5s and self.s_pos value if there is new Rover.image in supporting_functions.py - update_rover()\n",
    "\n",
    "## Important perception.py functions\n",
    "- def perspect_transform(img, src, dst): Do image perspective for color thresholding\n",
    "- def rock_thresh(wp_img): get rock samples from image\n",
    "- def color_thresh(img, rgb_thresh=(160, 160, 160)): get navigation area from image\n",
    "- Obstacle area = 1 - Navigation area\n",
    "- def rover_coords(binary_img): get the rover centric coordinates\n",
    "- def to_polar_coords(x_pixel, y_pixel): get the radial coordinates for rover\n",
    "- def rotate_pix, translate_pix, pix_to_world: Translate current Rover Yaw & Rover Position to the worldmap coordinates\n",
    "- def radar_scan(x_navi_rc,y_navi_rc,x_obs_rc,y_obs_rc): The Radar Scan function to check navigation pixels & obstacle pixels ahead 3 meters of rover navigation area\n",
    "- def perception_step(Rover): do basic image perception and update the Rover status \n",
    "\n",
    "# Confusion, Question, Conclusion for future improvement\n",
    "1. The best way for Rover jam unexpectly more than 5 sends is going backward, but I didn't find the method of Rover backward\n",
    "2. By Radar Scan function, we can improve rover decision more\n",
    "3. By image border identification, with rover size, obstacle size & rock sample size calculation in the image, the rover can pass through very narrow place\n",
    "4. Record Rover starting point coordinates & crossroad coordinates, the Rover can easily traverse the worldmap, pickup the rock sample and return to the starting place\n",
    "5. I find the HSV lower value & HSV upper value from Internet, the static value to identify yellow rock sample. \n",
    "but by [Object Tracking](http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html) , the calculation value is Yellow HSV value=[[[ 30 255 255]]], __then how to find the HSV lower&upper value for yellow rock?__\n",
    "\n",
    "# Reference\n",
    "1. [OpenCV 2.3.2 documentation](http://www.opencv.org.cn/opencvdoc/2.3.2/html/index.html)  \n",
    "2. [Geometric Image Transformations](http://docs.opencv.org/modules/imgproc/doc/geometric_transformations.html)  \n",
    "3. [Image Thresholding theory](http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html)  \n",
    "[Chinese version](http://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/tutorials/imgproc/threshold/threshold.html)  \n",
    "4. [Rotation matrix](https://en.wikipedia.org/wiki/Rotation_matrix)  \n",
    "5. [Telemetry](https://en.wikipedia.org/wiki/Telemetry)\n",
    "6. [SocketIO](https://python-socketio.readthedocs.io/en/latest/)\n",
    "7. [Polar coordinate system](https://en.wikipedia.org/wiki/Polar_coordinate_system)\n",
    "8. [Geometry translation](https://en.wikipedia.org/wiki/Translation_(geometry))\n",
    "9. [Median filter](https://en.wikipedia.org/wiki/Median_filter)\n",
    "10. [Frame of reference](https://en.wikipedia.org/wiki/Frame_of_reference)\n",
    "11. [Ipython buildin magic commands](http://ipython.readthedocs.io/en/stable/interactive/magics.html)\n",
    "12. [Python debugger](https://docs.python.org/3/library/pdb.html)\n",
    "13. [Matplotlib](http://matplotlib.org/faq/usage_faq.html#what-is-a-backend)\n",
    "14. [Robot garden waiver](http://www.robotgarden.org/get-involved/waiver/)\n",
    "15. [Buzz swarm robot](http://the.swarming.buzz/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RoboND]",
   "language": "python",
   "name": "conda-env-RoboND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
